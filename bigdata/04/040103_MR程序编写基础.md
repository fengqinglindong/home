## 数据需求

	用户每天在网站上的各种行为，会被网站记录下来，形成用户日志，并存储在HDFS上，日志格式如下：

```
2018-05-29 17:03:20.586ᄑpageviewᄑ{"device_id":"4405c39e85274857bbef58e013a08859","user_id":"0921528165741295","ip":"61.53.69.195","session_id":"9d6dc377216249e4a8f33a44eef7576d","req_url":"http://www.bigdataclass.com/my/0921528165741295"}
```
	
	时间戳，行为名称，json字符串（标识唯一的设备，用户登录后的id号，IP，会话，网页页面）。

	将这些字段解析成可以被数据仓库读取的序列化格式。最简单的，存成JSON格式。 

## 编写第一个MR程序

### 前提准备

	* IntelliJ IDEA Java 开发工具

	* Xshell SSH 连接

	* FileZilla FTP工具

	* Maven 项目管理工具
	
	本地测试环境：
	
		- Java 环境：jdk
		- Hadoop 本地环境：www.apache.org
		- Windows 下的hadoop环境: 
		- 测试日志数据：gitlab.com


### 新建Maven工程

	1. 用IDEA工具新建一个Maven工程
	2. 配置 [Pom.xml](/040103_pom.xml)，配置信息如下：
	<?xml version="1.0" encoding="UTF-8"?>
	<project xmlns="http://maven.apache.org/POM/4.0.0"
	         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	    <modelVersion>4.0.0</modelVersion>
	
	    <groupId>netease.bigdata.course</groupId>
	    <artifactId>etl</artifactId>
	    <version>1.0-SNAPSHOT</version>
	
	    <dependencies>
	        <dependency>
	            <groupId>org.apache.hadoop</groupId>
	            <artifactId>hadoop-client</artifactId>
	            <version>2.7.6</version>
	            <scope>provided</scope>
	        </dependency>
	        <dependency>
	            <groupId>com.alibaba</groupId>
	            <artifactId>fastjson</artifactId>
	            <version>1.2.4</version>
	        </dependency>
	    </dependencies>
	    <build>
	        <sourceDirectory>src/main</sourceDirectory>
	        <plugins>
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-assembly-plugin</artifactId>
	                <configuration>
	                    <descriptorRefs>
	                        <descriptorRef>
	                            jar-with-dependencies
	                        </descriptorRef>
	                    </descriptorRefs>
	                </configuration>
	                <executions>
	                    <execution>
	                        <id>make-assembly</id>
	                        <phase>package</phase>
	                        <goals>
	                            <goal>single</goal>
	                        </goals>
	                    </execution>
	                </executions>
	            </plugin>
	        </plugins>
	    </build>
	</project>

	3. 执行mvn clean package
	4. 编写ParseLogJob.java
	package com.bigdata.etl.job;
	
	import com.alibaba.fastjson.JSON;
	import com.alibaba.fastjson.JSONObject;
	import org.apache.commons.lang.StringUtils;
	import org.apache.hadoop.conf.Configuration;
	import org.apache.hadoop.conf.Configured;
	import org.apache.hadoop.fs.FileSystem;
	import org.apache.hadoop.fs.Path;
	import org.apache.hadoop.io.LongWritable;
	import org.apache.hadoop.io.NullWritable;
	import org.apache.hadoop.io.Text;
	
	import org.apache.hadoop.mapreduce.Job;
	import org.apache.hadoop.mapreduce.Mapper;
	import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
	import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
	import org.apache.hadoop.util.Tool;
	import org.apache.hadoop.util.ToolRunner;
	
	import java.io.IOException;
	import java.text.ParseException;
	import java.text.SimpleDateFormat;
	
	public class ParseLogJob extends Configured implements Tool {
    public static Text parseLog(String row) throws ParseException {
	        String[] logPart = StringUtils.split(row,"\u1111");
	        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS" );
	        long timeTag = dateFormat.parse(logPart[0]).getTime();
	        String activeName = logPart[1];
	        JSONObject bizData = JSON.parseObject(logPart[2]);
	
	        JSONObject logData = new JSONObject();
	        logData.put("active_name",activeName);
	        logData.put("time_tag",timeTag);
	        logData.putAll(bizData);
	
	        return new Text(logData.toJSONString());
	    }
	
	
	
	    public static class LogMapper extends Mapper<LongWritable, Text, NullWritable,Text> {
	        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException {
	            try {
	                Text parsedLog = parseLog(value.toString());
	                context.write(null,parsedLog);
	            } catch (ParseException e) {
	                e.printStackTrace();
	            }
	        }
	    }
	
	    public int run(String[] args) throws Exception {
	        Configuration config = getConf();
	        Job job = Job.getInstance(config);
	        job.setJarByClass(ParseLogJob.class);
	        job.setJobName("parselog");
	        job.setMapperClass(LogMapper.class);
	        job.setNumReduceTasks(0);
	
	        FileInputFormat.addInputPath(job ,new Path(args[0]));
	        Path outputPath = new Path(args[1]);
	        FileOutputFormat.setOutputPath(job,outputPath);
	
	        FileSystem fs = FileSystem.get(config);
	        if(fs.exists(outputPath)){
	            fs.delete(outputPath,true);
	        }
	        if(!job.waitForCompletion(true)){
	            throw new RuntimeException(job.getJobName()+"failed!"+args[1]+","+args[0]);
	        }
	        return 0;
	    }
	    public static void main(String[] args) throws Exception {
	        int res =ToolRunner.run(new Configuration(),new ParseLogJob(),args);
	        System.exit(res);
	    }
	}
