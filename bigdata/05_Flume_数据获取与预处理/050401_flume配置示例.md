## 示例一
	监听端口，输出到控制台
		//netcat指定从一个端口监听数据，从端口获取到以换行符为分割的文本数据，转换为flume event 结构
		Source.type=netcat
		//memory内存
		Channel.type = memory
		//
		Sink.type = logger

		./bin/flume-ng agent --conf conf --conf-file conf/conf.properties --name agent -Dflume.root.logger=INFO,console

	编写conf文件
		1. 进入conf目录，复制flume-conf.properties.template
			* cp flume-conf.properties.template flume-conf-netsrc.properties
		2. 编写flume-conf-netsrc.properties
			* agent.sources = netSrc
			* agent.channels = memoryChannel
			* agent.sinks = loggerSink
			
			# For each one of the sources, the type is defined
			* agent.sources.netSrc.type = netcat
			* agent.sources.netSrc.bind = 0.0.0.0 表明监听哪个服务器
			* agent.sources.netSrc.port = 12345 表明监听哪个服务器的端口
			# The channel can be defined as follows.
			* agent.sources.netSrc.channels = memoryChannel
			
			# Each sink's type must be defined
			* agent.sinks.loggerSink.type = logger
			
			#Specify the channel the sink should use
			* agent.sinks.loggerSink.channel = memoryChannel
			
			# Each channel's type is defined.
			* agent.channels.memoryChannel.type = memory
			
			# Other config values specific to each type of channel(sink or source)
			# can be defined as well
			# In this case, it specifies the capacity of the memory channel
			* agent.channels.memoryChannel.capacity = 100

		3. 输入启动命令
			*  ./bin/flume-ng agent --conf conf --conf-file conf/flume-conf-netsrc.properties --name agent -Dflume.root.logger=INFO,console
				*  其中，-Dflume.root.logger=INFO,console 是：将结果输出到控制台，同时指定了sinks类型为loggerSink，因此直接修改文件的配置属性，直接指定日志的配置属性，让flume进程直接启动log4j。
			*  再启动一个窗口，telnet到12345端口
				*  telnet localhost 12345
				*  输入hello world
			*  切回原来的窗口。

## 示例二
	监听端口输出到服务器文件
		//netcat指定从一个端口监听数据，从端口获取到以换行符为分割的文本数据，转换为flume event 结构
		Source.type=netcat
		//memory内存
		Channel.type = memory
		//将event数据存储到本地文件系统
		Sink.type = file_roll

		./bin/flume-ng agent --conf conf --conf-file conf/conf.properties --name agent -Dflume.root.logger=INFO,console
	
	编写conf文件
		1. 进入conf目录，复制flume-conf-netsrc.properties
			* cp flume-conf-netsrc.properties flume-conf-netsrc2localfile.properties
		2. 编写flume-conf-netsrc2localfile.properties
			* agent.sources = netSrc
			* agent.channels = memoryChannel
			* agent.sinks = fileSink
			
			# For each one of the sources, the type is defined
			* agent.sources.netSrc.type = netcat
			# 表明监听哪个服务器
			* agent.sources.netSrc.bind = 0.0.0.0 
			# 表明监听哪个服务器的端口
			* agent.sources.netSrc.port = 12345 
			# The channel can be defined as follows.
			* agent.sources.netSrc.channels = memoryChannel
			
			# Each sink's type must be defined
			* agent.sinks.fileSink.type = file_roll
			# 将生产的文件保存到哪个路径下，这个路径必须存在
			* agent.sinks.fileSink.directory = /mnt/home/11705048/workspace/file_roll/data
			# 间隔多久生成一个文件
			* agent.sinks.fileSink.rollInterval = 60
			
			#Specify the channel the sink should use
			* agent.sinks.fileSink.channel = memoryChannel
			
			# Each channel's type is defined.
			* agent.channels.memoryChannel.type = memory
			
			# Other config values specific to each type of channel(sink or source)
			# can be defined as well
			# In this case, it specifies the capacity of the memory channel
			* agent.channels.memoryChannel.capacity = 100
		3. 返回上级目录创建/mnt/home/11705048/workspace/file_roll/data
			- mkdir -p workspace/file_roll/data
		4. 输入启动命令
			* ./bin/flume-ng agent --conf conf --conf-file conf/flume-conf-netsrc2localfile.properties --name agent 
			*  再启动一个窗口，telnet到12345端口,向其中数量数据
				*  telnet localhost 12345
				*  输入hello world
			* 再启动一个窗口，打开工作目录workspace/file_roll/data
			* 打开文件 less 文件名

## 示例三
	监听服务器文件，输出到hdfs
		//
		Source.type=taildir
		Channel.type = memory
		Sink.type = hdfs

		修改flume-env.sh
		export HADOOP_HOME=/home/hadoop/hadoop-current
		FLUME_CLASSPATH="$HADOOP_HOME/bin:$PATH"

		./bin/flume-ng agent --conf conf --conf-file conf/conf.properties --name a1

	编写conf文件
		1. 进入conf目录，复制flume-conf.properties.template
			* cp flume-conf.properties.template flume-conf-taildir2hdfs.properties
		2. 编写flume-conf-taildir2hdfs.properties
			* agent.sources = fileSrc
			* agent.channels = memoryChannel
			* agent.sinks = hdfsSink
			
			# For each one of the sources, the type is defined
			* agent.sources.fileSrc.type = taildir
			# 指定文件路径，保存js格式的文件,这个文件记录了每个监听文件的Ilod以及这个文件所在的路径以及上一次的读取位置
			* agent.sources.fileSrc.positionFile = /mnt/home/11705048/workspace/hdfs_sink/mem_ch/positionFile
			# 指定了一系列的文件组，这些组通过空格分割，每文件组指向一组被监听的文件
			* agent.sources.fileSrc.filegroups= f1
			# 指定了文件组的绝对路径
			* agent.sources.fileSrc.filegroups.f1= /mnt/hadoop/log/.*.log
			# The channel can be defined as follows.
			* agent.sources.fileSrc.filegroups = memoryChannel
			
			# Each sink's type must be defined
			* agent.sinks.hdfsSink.type = hdfs
			# 将数据保存在分布式文件系统的目录
			* agent.sinks.hdfsSink.hdfs.path = /user/11705048/flume_data/mem_ch/%y-%m-%d
			# sink读取这个文件的时间戳来替换转意符
			* agent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true
			# DataStream不使用压缩
			* agent.sinks.hdfsSink.hdfs.fileType = DataStream
			# 压缩格式
			# agent.sinks.hdfsSink.hdfs.codeC = gizp/lzo/snappy
			# 写入到hdfs上时怎么分割
			* agent.sinks.hdfsSink.hdfs.rollSize = 0
			* agent.sinks.hdfsSink.hdfs.rollCount = 0
			* agent.sinks.hdfsSink.hdfs.rollInterval = 60
			
			#Specify the channel the sink should use
			* agent.sinks.hdfsSink.channel = memoryChannel
			
			# Each channel's type is defined.
			* agent.channels.memoryChannel.type = memory
			
			# Other config values specific to each type of channel(sink or source)
			# can be defined as well
			# In this case, it specifies the capacity of the memory channel
			* agent.channels.memoryChannel.capacity = 100
		3. 返回家目录，修改.bashrc文件
			- export HADOOP_HOME=/home/hadoop/hadoop-current
			- export PATH=$HASOOP_HOME/bin:$MAVEN_HOME/bin:$PATH
			- 验证hadoop的目录
				- echo $HADOOP_HOME
		4. 输入启动命令
			* ./bin/flume-ng agent --conf conf --conf-file conf/flume-conf-taildir2hdfs.properties --name agent 
			*  再启动一个窗口，进入/user/11705048/flume_data/mem_ch
				*  hadoop fs -ls /user/11705048/flume_data/mem_ch
				*  hadoop fs -ls /user/11705048/flume_data/mem_ch/日期目录

## 示例四
	监听服务器文件，输出到hdfs（file channel）
		//
		Source.type=taildir
		Channel.type = file
		Sink.type = hdfs
	
		./bin/flume-ng agent --conf conf --conf-file conf/conf.properties --name a1

	编写conf文件
		1. 进入conf目录，复制flume-conf-taildir2hdfs.properties
			* cp flume-conf-taildir2hdfs.properties flume-conf-taildir2hdfs-filech.properties
		2. 编写flume-conf-taildir2hdfs-filech.properties
			* agent.sources = fileSrc
			* agent.channels = fileChannel
			* agent.sinks = hdfsSink
			
			# For each one of the sources, the type is defined
			* agent.sources.fileSrc.type = taildir
			# 指定文件路径，保存js格式的文件,这个文件记录了每个监听文件的Ilod以及这个文件所在的路径以及上一次的读取位置
			* agent.sources.fileSrc.positionFile = /mnt/home/11705048/workspace/hdfs_sink/file_ch/positionFile
			# 指定了一系列的文件组，这些组通过空格分割，每文件组指向一组被监听的文件
			* agent.sources.fileSrc.filegroups= f1
			# 指定了文件组的绝对路径
			* agent.sources.fileSrc.filegroups.f1= /mnt/hadoop/log/.*.log
			# The channel can be defined as follows.
			* agent.sources.fileSrc.channels = fileChannel
			
			# Each sink's type must be defined
			* agent.sinks.hdfsSink.type = hdfs
			# 将数据保存在分布式文件系统的目录
			* agent.sinks.hdfsSink.hdfs.path = /user/11705048/flume_data/file_ch/%y-%m-%d
			# sink读取这个文件的时间戳来替换转意符
			* agent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true
			# DataStream不使用压缩
			* agent.sinks.hdfsSink.hdfs.fileType = DataStream
			# 压缩格式
			# agent.sinks.hdfsSink.hdfs.codeC = gizp/lzo/snappy
			# 写入到hdfs上时怎么分割
			* agent.sinks.hdfsSink.hdfs.rollSize = 0
			* agent.sinks.hdfsSink.hdfs.rollCount = 0
			* agent.sinks.hdfsSink.hdfs.rollInterval = 60
			
			#Specify the channel the sink should use
			* agent.sinks.hdfsSink.channel = fileChannel
			
			# Each channel's type is defined.
			* agent.channels.fileChannel.type = file
			* agent.channels.fileChannel.checkpointDir = /mnt/home/11705048/workspace/hdfs_sink/file_ch/checkpoint
			* agent.channels.fileChannel.checkpointInterval = 60000
			* agent.channels.fileChannel.dataDIrs = /mnt/home/11705048/workspace/hdfs_sink/file_ch/data
			
		3. 输入启动命令
			* ./bin/flume-ng agent --conf conf --conf-file conf/flume-conf-taildir2hdfs-filech.properties --name agent 
			*  再启动一个窗口，进入/user/11705048/flume_data
				*  hadoop fs -ls /user/11705048/flume_data
				*  hadoop fs -ls /user/11705048/flume_data/file_ch/日期目录
				* 本地目录是否存在
					* /mnt/home/11705048/workspace/hdfs_sink/file_ch
