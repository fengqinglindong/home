## Import - 数据导入 ##
语法样例：

	./sqoop import --connect "jdbc:mysql://*.*.*.*:protNum/DbName?characterEncoding=UTF-8&useCursorFetch=true" --username *** --password *** --query "SELECT * FROM HIVE_JDBC_TEST2 WHERE id > 2 AND \$CONDITIONS" --split-by id --target-dir "/user/hadoop/HIVE_JDBC_TEST2_QUERY" --delete-target-dir --as-textfile
参数说明：  



- --connect：指定JDBC连接数据库所在的地址、端口、数据库名称、字符编码格式等信息

- --username：数据库连接用户名

- --password：数据库连接密码

- --query：获取导入数据的查询语句

- --split-by：指定分片字段，后续会使用改字段对数据进行换分，放到不同的map中执行，一般使用主键或者有索引的字段，可提高执行效率

- --target-dir：指定HDFS上的目标路径，即导入数据的存储路径

- --delete-target-dir：如果目标文件存在就先删除在导入

- --as-textfile：指定导入数据到HDFS后的数据存储格式。

	- 支持有text、sequence、avro、parquet
- --boudary-query：指定数据边界查找语句。sqoop默认使用select min(), max() from 来查找split的总边界。但是这种自动生成的查询语句执行效率并不高。

- -m/--num-mappers指定Map个数。这个参数会显著影响程序的执行效率，太小的话并发数不够，程序执行效率低，太大的话任务的启动开销过大，任务执行也会受影响。

- --table,column,where分别对应于Table，Column，Where限制条件。

- --compress，--compression-codec：使用压缩

- --map-column-java：指定Column在Sqoop对象中的数据类型，一般不需要手动添加。

- 增量导入（incremental）：指定关系数据库的某个Column和last value，下次导入的时候只导入比last value大的数据。增量导入分两种模式：

	- append：把新数据追加到目标路径中

	- last modified：会将新数据和老数据进行合并，合并的话需要新老数据的对应关系，所以还需要添加参数merge key来指定主键，最后执行结果只保留最新数据。该模式下last value的值一定要是timestamp或者data.

## Hive import ##
Hive import 任务：

	./sqoop import --connect "jdbc:mysql://*.*.*.*:protNum/DbName?characterEncoding=UTF-8&useCursorFetch=true" --username *** --password *** --table sqooptest --split-by id --target-dir "/user/hadoop/sqooptest" --delete-target-dir --hive-import --hive-table sqooptest map-column-hive id=INT,name=STRING

参数增加了设置hive的操作：  

- --hive-import
	- 表示开启hive import功能
- --hive-table
	- 指定导入hive中table的名字
- map-column-hive
	- 定义hive中table的字段类型
- 支持hive table的创建与覆盖：create-hive-table, hive-overwritess  
	- 默认会判断hive table是否已经存在，如果存在就创建否则跳过。还可以强制执行建表语句，有上面参数
- 支持hive分区：hive-partition-key,hive-partition-value
- map-column-hive:指定hive column的数据类型。例如key=value。限制：Hive Column顺序需等同与sqoop导入的hdfs文件column顺序，否则会导入失败（可在-column或者-query中指定column顺序）
- 支持指定查询语句，解决多表关联插入。

## Export - 数据导出 ##

	sqoop export --connect jdbc:mysql://10.172.121.103:3306/test?characterEncoding=UTF-8 --username mammoth_test --password VwvV8r37ccrR --table mysql_base_export -columns id,name --export-dir /user/hadoop/mysql_base_no_meta

- table,columns:指定导出rdms中的table，columns
- 支持sequence，avro，parquet导出。其中sequence，avro，parquet格式导出sqoop可自动识别
- update-mode:分为updateonly和allowinsert两种类型。allowinsert可进行update insert操作
- update-key：根据指定列，完成update

## RDMS工具 ##

	sqoop list-databases --connect jdbc:mysql://*.*.*:3306/dbName?characterEncoding=UTF-8 --username mammoth_test --password VwvV8r37ccrR

rdms结构：
- list-databases:列出所有数据库
- list-tables：列出所有表

	sqoop eval --connect jdbc:mysql://*.*.*:3306/dbName?characterEncoding=UTF-8 --username mammoth_test --password VwvV8r37ccrR -e "select * from ..."

查看，修改rdms数据
- -e,--query:执行指定query

